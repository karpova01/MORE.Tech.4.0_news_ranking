{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7TWROHFoGQc"
      },
      "outputs": [],
      "source": [
        "pip install feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# источники\n",
        "our_feeds = {'Klerk': 'https://www.klerk.ru/export/news.rss',\n",
        "'Glavbukh': 'https://www.glavbukh.ru/rss/news.xml',\n",
        "'Buh':'https://buh.ru/rss/?chanel=articles',\n",
        "'Garant': 'http://rss.garant.ru/categories/news/',\n",
        "'Nalogi' : 'http://rss.garant.ru/consult/nalog/',\n",
        "'MK' : 'https://www.mk.ru/rss/economics/index.xml'} \n",
        "\n",
        "#пример пути файла\n",
        "f_all_news = 'allnews23march.csv' \n",
        "f_certain_news = 'certainnews23march.csv'\n",
        "\n",
        "#пример таргетов\n",
        "vector1 = 'ДолЛАР|РубЛ|ЕвРО'\n",
        "vector2 = 'ЦБ|СбЕРбАНК|курс'\n",
        "\n",
        "def check_url(url_feed): #функция получает линк на рсс ленту, возвращает        \n",
        "# распаршенную ленту с помощью feedpaeser\n",
        "    return feedparser.parse(url_feed)  \n",
        "    \n",
        "def getHeadlines(url_feed): #функция для получения заголовков новости\n",
        "    headlines = []\n",
        "    lenta = check_url (url_feed)\n",
        "    for item_of_news in lenta['items']:\n",
        "        headlines.append(item_of_news ['title'])\n",
        "    return headlines\n",
        "\n",
        "def getDescriptions(url_feed): #функция для получения описания новости\n",
        "    descriptions = []\n",
        "    lenta = check_url(url_feed)\n",
        "    for item_of_news in lenta['items']:\n",
        "        descriptions.append(item_of_news ['description'])\n",
        "    return descriptions\n",
        "\n",
        "def getLinks(url_feed): #функция для получения ссылки на источник новости\n",
        "    links = []\n",
        "    lenta = check_url(url_feed)\n",
        "    for item_of_news in lenta['items']:\n",
        "        links.append(item_of_news ['link'])\n",
        "    return links\n",
        "\n",
        "def getDates(url_feed): #функция для получения даты публикации новости\n",
        "    dates = []\n",
        "    lenta = check_url(url_feed)\n",
        "    for item_of_news in lenta['items']:\n",
        "        dates.append(item_of_news ['published'])\n",
        "    return dates\n",
        "\n",
        "allheadlines = []\n",
        "alldescriptions = []\n",
        "alllinks = []\n",
        "alldates = []\n",
        "\n",
        "# Прогоняем URL и добавляем их в пустые списки\n",
        "for key,url in our_feeds.items():\n",
        "    allheadlines.extend( getHeadlines(url) )\n",
        "    \n",
        "for key,url in our_feeds.items():\n",
        "    alldescriptions.extend( getDescriptions(url) )\n",
        "    \n",
        "for key,url in our_feeds.items():\n",
        "    alllinks.extend( getLinks(url) )\n",
        "    \n",
        "for key,url in our_feeds.items():\n",
        "    alldates.extend( getDates(url) )\n",
        "\n",
        "def write_all_news(all_news_filepath): #функция для записи всех новостей в .csv, \n",
        "# возвращает нам этот датасет\n",
        "    header = ['Title','Description','Links','Publication Date'] \n",
        "\n",
        "    with open(all_news_filepath, 'w', encoding='utf-8-sig') as csvfile:\n",
        "        writer = csv.writer(csvfile, delimiter=',')\n",
        "\n",
        "        writer.writerow(i for i in header) \n",
        "\n",
        "        for a,b,c,d  in zip(allheadlines,alldescriptions,\n",
        "                            alllinks, alldates):\n",
        "            writer.writerow((a,b,c,d))\n",
        "\n",
        "\n",
        "        df = pd.read_csv(all_news_filepath)\n",
        "        \n",
        "            \n",
        "    return df\n",
        "\n",
        "def looking_for_certain_news(all_news_filepath, certain_news_filepath, target1, target2): #функция для поиска, а затем записи\n",
        "                #определенных новостей по таргета,\n",
        "                #затем возвращает этот датасет\n",
        "    df = pd.read_csv(all_news_filepath)\n",
        "    \n",
        "    result = df.apply(lambda x: x.str.contains(target1, na=False,\n",
        "                                    flags = re.IGNORECASE, regex=True)).any(axis=1)\n",
        "    result2 = df.apply(lambda x: x.str.contains(target2, na=False,\n",
        "                                    flags = re.IGNORECASE, regex=True)).any(axis=1)\n",
        "    new_df = df[result&result2]\n",
        "        \n",
        "    new_df.to_csv(certain_news_filepath\n",
        "                     ,sep = '\\t', encoding='utf-8-sig')\n",
        "        \n",
        "    return new_df\n",
        "\n",
        "df = write_all_news(f_all_news) #все новости\n",
        "df.to_csv(r'/content/export_dataframe.csv', index = False, header=True, encoding='utf-8')\n",
        "\n",
        "looking_for_certain_news(f_all_news, f_certain_news, vector1, vector2) "
      ]
    }
  ]
}