{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0f39dc19-096e-45a5-a299-55d360b124eb",
      "metadata": {
        "id": "0f39dc19-096e-45a5-a299-55d360b124eb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL = 'https://www.audit-it.ru/archive/'\n",
        "rg = 'https://www.audit-it.ru'\n",
        "page = requests.get(URL)\n",
        "\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "results = soup.find_all(class_=\"archive_year\")\n",
        "news = []\n",
        "\n",
        "for result in results:\n",
        "    data = result.find(class_=\"archive_block_title\")\n",
        "    if (data.getText() in [\"2022\"]):\n",
        "        for newsFound in result.find_all('a', href=True):\n",
        "            news.append(newsFound['href'])\n",
        "\n",
        "sites = []\n",
        "for newsSite in news:\n",
        "    page_news = requests.get(rg + newsSite)\n",
        "    news_soup = BeautifulSoup(page_news.content, \"html.parser\")\n",
        "    for textNews in news_soup.find_all(class_=\"archive_line\"):\n",
        "        for site in textNews.find_all('a', href=True):\n",
        "            sites.append(site['href'])\n",
        "\n",
        "listOfNews = []\n",
        "for newsSite in sites:\n",
        "    page_news = requests.get(rg + newsSite, headers = {'User-agent': 'Super Bot Power Level Over 9000'})\n",
        "    \n",
        "    news_soup = BeautifulSoup(page_news.content, \"html.parser\")\n",
        "    for textNews in news_soup.find_all(class_=\"news-title-box\"):\n",
        "        for site in textNews.find_all('a', href=True):\n",
        "            listOfNews.append(site['href'])\n",
        "\n",
        "list_of_lists = []\n",
        "for newsSite in listOfNews:\n",
        "    page_news = requests.get(rg + newsSite, headers = {'User-agent': 'Super Bot Power Level Over 9000'})\n",
        "    news_soup = BeautifulSoup(page_news.content, \"html.parser\")\n",
        "    if ('articles' in newsSite):\n",
        "        textPage = news_soup.find(class_ = 'article-text')\n",
        "    elif ('law'in newsSite):\n",
        "        textPage = news_soup.find(class_ = 'article-text')\n",
        "    else:\n",
        "        textPage = news_soup.find(class_ = 'news-text')\n",
        "    a = listOfNews.index(newsSite)\n",
        "    s = rg + newsSite\n",
        "    maintitle = news_soup.find(class_ = 'maintitle')\n",
        "    if maintitle: maintitle = maintitle.text\n",
        "    datePublished = news_soup.find(itemprop='datePublished').text\n",
        "    \n",
        "    alltext = []\n",
        "    for text in textPage.find_all('p'):\n",
        "        alltext.append(text.text)\n",
        "    list_of_lists.append([maintitle, datePublished, alltext, rg + newsSite])\n",
        "df = pd.DataFrame(list_of_lists, columns=['Title', 'Date', 'Text', 'Site'])\n",
        "\n",
        "df.to_csv (r'/content/export_dataframe.csv', index = False, header=True, encoding='utf-8')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}